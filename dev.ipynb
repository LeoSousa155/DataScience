{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Descrição do problema\n",
    "\n",
    "No âmbito da unidade curricular de Ciência de Dados, foi proposto o desenvolvimento de um modelo de machine learning utilizando o dataset \"New York City Taxi Trips\". O objetivo é construir um modelo capaz de prever o preço das viagens de táxi com base em diversas variáveis disponíveis, como distância da viagem, horário do dia, número de passageiros e possíveis condições de tráfego.\n",
    "\n",
    "# Objetivo\n",
    "O principal objetivo deste projeto é desenvolver um modelo que consiga estimar o valor das tarifas de táxi em Nova York. A tarefa será abordada sob duas perspectivas:\n",
    "\n",
    "1. Regressão: Criar um modelo que consiga prever com precisão o valor exato da tarifa para uma determinada viagem.\n",
    "2. Classificação: Reformular o problema para classificar as viagens em faixas de preços predefinidas:\n",
    "\n",
    "    - Classe 1: Viagens curtas e de baixo custo (`< $10`)\n",
    "\n",
    "    - Classe 2: Viagens de média distância e preço moderado (`$10 - $30`)\n",
    "\n",
    "    - Classe 3: Viagens longas com tarifas mais elevadas (`$30 - $60`)\n",
    "\n",
    "    - Classe 4: Tarifas premium (`> $60`)\n",
    "\n",
    "# Descrição dos Dados\n",
    "\n",
    "O dataset **New York City Yellow Taxi Trip Records** contém informações detalhadas sobre viagens de táxi na cidade de Nova York, incluindo dados sobre tempo, distância, localizações de embarque e desembarque, além de informações de pagamento.\n",
    "\n",
    "O dataset inclui as seguintes colunas:\n",
    "\n",
    "- **VendorID**: Código que identifica o provedor do sistema de processamento eletrônico de pagamentos (TPEP).\n",
    "  - 1 = Creative Mobile Technologies, LLC\n",
    "  - 2 = VeriFone Inc.\n",
    "\n",
    "- **tpep_pickup_datetime**: Data e hora em que o taxímetro foi ativado.\n",
    "- **tpep_dropoff_datetime**: Data e hora em que o taxímetro foi desativado.\n",
    "- **Passenger_count**: Número de passageiros no veículo (valor inserido pelo motorista).\n",
    "- **Trip_distance**: Distância percorrida na viagem (em milhas) reportada pelo taxímetro.\n",
    "- **PULocationID**: Código da zona TLC onde o taxímetro foi ativado (embarque).\n",
    "- **DOLocationID**: Código da zona TLC onde o taxímetro foi desativado (desembarque).\n",
    "- **RateCodeID**: Código da tarifa aplicada ao final da viagem.\n",
    "  - 1 = Tarifa padrão\n",
    "  - 2 = JFK\n",
    "  - 3 = Newark\n",
    "  - 4 = Nassau ou Westchester\n",
    "  - 5 = Tarifa negociada\n",
    "  - 6 = Viagem em grupo\n",
    "\n",
    "- **Store_and_fwd_flag**: Indica se o registro da viagem foi armazenado antes de ser enviado ao provedor, devido à falta de conexão do veículo com o servidor.\n",
    "  - Y = Viagem armazenada antes do envio\n",
    "  - N = Viagem enviada em tempo real\n",
    "\n",
    "- **Payment_type**: Código que indica a forma de pagamento utilizada pelo passageiro.\n",
    "  - 1 = Cartão de crédito\n",
    "  - 2 = Dinheiro\n",
    "  - 3 = Sem cobrança\n",
    "  - 4 = Disputa\n",
    "  - 5 = Desconhecido\n",
    "  - 6 = Viagem cancelada\n",
    "\n",
    "- **Fare_amount**: Valor da tarifa baseado no tempo e na distância percorrida.\n",
    "- **Extra**: Cobranças adicionais, como sobretaxas noturnas ou de horário de pico.\n",
    "- **MTA_tax**: Taxa de `$0,50` do MTA aplicada automaticamente com base na tarifa do taxímetro.\n",
    "- **Improvement_surcharge**: Taxa de melhoria de `$0,30` aplicada desde 2015.\n",
    "- **Tip_amount**: Valor da gorjeta (preenchido automaticamente para pagamentos com cartão; gorjetas em dinheiro não são registradas).\n",
    "- **Tolls_amount**: Valor total de pedágios pagos durante a viagem.\n",
    "- **Total_amount**: Valor total cobrado do passageiro (não inclui gorjetas pagas em dinheiro).\n"
   ],
   "id": "66a1dd6e3ef3af8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:23:59.475509Z",
     "start_time": "2025-03-09T16:23:58.484253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3 as sql\n",
    "import kagglehub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n"
   ],
   "id": "195c52c539317959",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usoda\\anaconda3\\envs\\Project_dependencies\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Analysis\n",
    "\n",
    "A análise de dados será realizada com a abordagem ELT (Extract, Load, Transform). Inicialmente, os dados serão extraídos do KaggleHub e uma amostra aleatória será utilizada para trabalhar com uma fração representativa do conjunto completo. Em seguida, os dados extraídos serão carregados em estruturas como DataFrames Pandas ou arrays NumPy, facilitando a manipulação e análise. Por fim, na fase de transformação, os dados serão limpos, normalizados e preparados para gerar métricas e insights. A amostragem aleatória permite reduzir a complexidade, permitindo uma análise eficiente sem processar todos os dados de uma vez."
   ],
   "id": "36ab584fc91a2de7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:24:02.426919Z",
     "start_time": "2025-03-09T16:24:02.407385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KagglehubDatabaseLoader:\n",
    "    \"\"\"\n",
    "    A class to download and manage datasets from KaggleHub.\n",
    "\n",
    "    This class automates the process of downloading a dataset from KaggleHub\n",
    "    and provides a method to retrieve the local file path.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (str): The name of the dataset to be downloaded.\n",
    "        _path (str or None): The local directory path where the dataset is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str):\n",
    "        \"\"\"\n",
    "        Initializes the KagglehubDatabaseLoader class.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): The name of the dataset to be downloaded.\n",
    "        \"\"\"\n",
    "        self._dataset = dataset\n",
    "        self._path = None\n",
    "\n",
    "        self._download_dataset()\n",
    "\n",
    "\n",
    "    def _download_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Downloads the specified dataset from KaggleHub.\n",
    "\n",
    "        This method attempts to download the dataset and assigns the local\n",
    "        file path to the `_path` attribute. If an error occurs during the\n",
    "        download, an exception is caught and printed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._path = kagglehub.dataset_download(self._dataset)\n",
    "            print(\"Dataset downloaded Successfully \")\n",
    "            print(\"Dataset saved on: \", self._path)\n",
    "        except Exception as e:\n",
    "            print(\"Error downloading dataset:\", e)\n",
    "\n",
    "\n",
    "    def get_path(self) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns the local directory path of the downloaded dataset.\n",
    "\n",
    "        Returns:\n",
    "            str or None: The file path of the dataset if the download was\n",
    "                        successful, otherwise None.\n",
    "        \"\"\"\n",
    "        return self._path\n",
    "\n",
    "\n",
    "class KagglehubSQLiteLoader(KagglehubDatabaseLoader):\n",
    "    \"\"\"\n",
    "    A subclass of KagglehubDatabaseLoader to interact with an SQLite dataset.\n",
    "\n",
    "    This class adds methods to retrieve information about tables, numpy arrays and dataframes.\n",
    "    the SQLite dataset is downloaded from KaggleHub.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (str): The name of the dataset to be downloaded from KaggleHub.\n",
    "        _path (str or None): The local directory path where the dataset is saved.\n",
    "        _file_path (str or None): The complete path to the specific SQLite file within the dataset.\n",
    "        _conn: The SQLite connection object used to interact with the database.\n",
    "        _cursor: The cursor object for executing SQL queries on the database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str, file: str):\n",
    "        \"\"\"\n",
    "        Initializes the KagglehubSQLLoader class by calling the superclass constructor\n",
    "        to download the dataset and setting up the SQLite connection.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): The dataset name to be downloaded.\n",
    "            file (str): The name of the SQLite file in the downloaded dataset to be opened.\n",
    "        \"\"\"\n",
    "        super().__init__(dataset)\n",
    "        self._file_path = self._path + file\n",
    "        self._conn = sql.connect(self._file_path)\n",
    "        self._cursor = self._conn.cursor()\n",
    "\n",
    "\n",
    "    def get_table_names(self) -> list | None:\n",
    "        \"\"\"\n",
    "        Retrieves all table names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of table names if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            table_name_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "            return self._cursor.execute(table_name_query).fetchall()\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table names:\", e)\n",
    "\n",
    "\n",
    "    def get_column_names(self, table: str) -> list | None:\n",
    "        \"\"\"\n",
    "        Retrieves the column names for a given table in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table for which to get column names.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of column names if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "            columns = [col[1] for col in self._cursor.fetchall()]\n",
    "            return columns\n",
    "        except Exception as e:\n",
    "            print(\"Error getting column names:\", e)\n",
    "\n",
    "\n",
    "    def get_table_row_count(self, table: str) -> int | None:\n",
    "        \"\"\"\n",
    "        Retrieves the row count for a given table.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table for which to get the row count.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of rows in the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT COUNT(*) FROM {table};\")\n",
    "            return self._cursor.fetchone()[0]\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table row count:\", e)\n",
    "\n",
    "\n",
    "    def get_table_data(self, table: str) -> np.ndarray | None:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a given table as a NumPy array.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array containing the data from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT * FROM {table}\")\n",
    "            data = self._cursor.fetchall()\n",
    "            column_names = self.get_column_names(table)\n",
    "            df = pd.DataFrame(data, columns=column_names)\n",
    "            return df.to_numpy()\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table data array:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe(self, table: str) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a given table as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the data from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT * from {table}\")\n",
    "            data = self._cursor.fetchall()\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table dataframe:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe_nrows(self, table: str, n: int) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves the first `n` rows of a given table as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "            n (int): The number of rows to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the first `n` rows from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query = \"SELECT * from tripdata\"\n",
    "            data = self._cursor.execute(query).fetchmany(n)\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting the first {n} rows from table:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe_random_sample(self, table: str, percentage: float = 0.05) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves a random sample of rows from the specified table based on a given percentage.\n",
    "\n",
    "        This method selects a random subset of the rows from the given table. The percentage of\n",
    "        rows to sample is specified by the `percentage` argument. The sampling is done using\n",
    "        the SQLite `RANDOM()` function, which provides a pseudo-random selection of rows.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table from which to retrieve the random sample.\n",
    "            percentage (float, optional): The fraction of rows to sample, between 0.0 and 1.0.\n",
    "                                          Defaults to 0.05 (5%).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: A Pandas DataFrame containing the random sample of rows.\n",
    "                                  Returns an empty DataFrame if no data is found, or None if an error occurs.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the `percentage` is not between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not 0.0 <= percentage <= 1.0:\n",
    "                raise ValueError(\"Percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "            size = self.get_table_row_count(table)\n",
    "            threshold = int(round(percentage * size))\n",
    "\n",
    "            query = \"SELECT * FROM tripdata WHERE ABS(RANDOM()) % ? < ?\"\n",
    "\n",
    "            self._cursor.execute(query, (size, threshold))\n",
    "            data = self._cursor.fetchall()\n",
    "\n",
    "            if not data:\n",
    "                return pd.DataFrame(columns=self.get_column_names(table))\n",
    "\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting random sample from table:\", e)"
   ],
   "id": "aed381c472c84667",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:55:49.103515Z",
     "start_time": "2025-03-09T16:55:49.088342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataAnalizer:\n",
    "    \"\"\"\n",
    "    A class to load and split a dataset into training and testing sets.\n",
    "\n",
    "    This class is designed to handle the process of dividing a DataFrame into\n",
    "    training and testing sets for machine learning tasks. It takes in a DataFrame,\n",
    "    a target column, and splits the data into features and labels, which are then\n",
    "    separated into training and testing datasets.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        target (str): The name of the target column in the dataset.\n",
    "        test_size (float): The proportion of the dataset to include in the test split (default is 0.2).\n",
    "        random_state (int or None): The random seed for reproducibility of the split (default is None).\n",
    "        data_train (pd.DataFrame): The feature data for the training set.\n",
    "        labels_train (pd.Series): The labels for the training set.\n",
    "        data_test (pd.DataFrame): The feature data for the testing set.\n",
    "        labels_test (pd.Series): The labels for the testing set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, target: str,  test_size= 0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader instance and splits the dataset into training and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset to be split.\n",
    "            target (str): The name of the target column in the dataset.\n",
    "            test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "            random_state (int or None, optional): The random seed used for shuffling the data. Default is None.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.data_train = None\n",
    "        self.labels_train = None\n",
    "        self.data_test = None\n",
    "        self.labels_test = None\n",
    "\n",
    "        #divide data in train and test sets\n",
    "        self._divide_data()\n",
    "\n",
    "\n",
    "    def _divide_data(self):\n",
    "        \"\"\"\n",
    "        Splits the DataFrame into features (X) and labels (y), and divides them into training and testing sets.\n",
    "\n",
    "        The method uses `train_test_split()` from scikit-learn to randomly split the data into\n",
    "        training and test sets based on the specified `test_size` and `random_state`.\n",
    "\n",
    "        The method sets the following attributes:\n",
    "            - data_train: Feature data for training.\n",
    "            - labels_train: Labels for training.\n",
    "            - data_test: Feature data for testing.\n",
    "            - labels_test: Labels for testing.\n",
    "        \"\"\"\n",
    "        x = self.df.drop(columns=[self.target])\n",
    "        y = self.df[self.target]\n",
    "        x_train, y_train, x_test, y_test = train_test_split(\n",
    "            x, y, test_size=self.test_size,random_state=self.random_state\n",
    "        )\n",
    "        self.data_train  = x_train\n",
    "        self.labels_train = y_train\n",
    "        self.data_test    = x_test\n",
    "        self.labels_test  = y_test\n",
    "        print(\"Data divided successfully.\")\n",
    "\n",
    "\n",
    "    def _drop_columns(self, cols):\n",
    "        \"\"\"\n",
    "        Drops a specified feature columns from both the training and testing datasets.\n",
    "\n",
    "        Args:\n",
    "            cols (str): The list of the columns to be dropped from both the training and testing datasets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_train.drop(cols, axis=1, inplace=True)\n",
    "            self.data_test.drop(cols, axis=1, inplace=True)\n",
    "            print(f\"Feature columns {cols} dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping columns {cols}:\", e)\n",
    "\n",
    "\n",
    "\n",
    "class TripDataManipulator(DataAnalizer):\n",
    "    \"\"\"\n",
    "    TripDataManipulator is a class that inherits from DataAnalizer and extends its functionality\n",
    "    to preprocess and engineer features specifically for trip data.\n",
    "\n",
    "    This class performs several data manipulation tasks, including:\n",
    "\n",
    "    -   Extracting datetime features from pickup and dropoff timestamps.\n",
    "    -   Calculating the trip duration in minutes.\n",
    "    -   Calculating the average speed of the trip in miles per hour.\n",
    "\n",
    "    It assumes the input DataFrame contains columns 'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime', and 'trip_distance'.\n",
    "\n",
    "    The class initializes with a DataFrame, target column, test size for train-test split,\n",
    "    and a random state for reproducibility. It then automatically calls methods to extract\n",
    "    datetime features, calculate trip duration, and calculate average speed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, target: str, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the TripDataManipulator instance and splits the dataset into training and test sets.\n",
    "        It also calls class specific methods of the class.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset to be split.\n",
    "            target (str): The name of the target column in the dataset.\n",
    "            test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "            random_state (int or None, optional): The random seed used for shuffling the data. Default is None.\n",
    "        \"\"\"\n",
    "\n",
    "        #Manipulate data\n",
    "        df = self.extract_datetime_features(df)\n",
    "        df = self.calculate_trip_duration(df)\n",
    "        df = self.calculate_average_speed(df)\n",
    "\n",
    "        super().__init__(df, target, test_size, random_state)\n",
    "\n",
    "\n",
    "\n",
    "    # Metodo para separar \"tpep_pickup_datetime\" e \"tpep_dropoff_datetime\" em várias features\n",
    "    def extract_datetime_features(self, df):\n",
    "        \"\"\"\n",
    "        Converts pickup and dropoff datetimes into a datetime object.\n",
    "        This method also creates usefull new features from this object to extract future insights about time\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure datetime columns are in the correct format\n",
    "        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "        # Extract features from pickup time\n",
    "        df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "        df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
    "        df['pickup_day_of_month'] = df['tpep_pickup_datetime'].dt.day\n",
    "        df['pickup_month'] = df['tpep_pickup_datetime'].dt.month\n",
    "\n",
    "        # Extract features from dropoff time\n",
    "        df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
    "        df['dropoff_day_of_week'] = df['tpep_dropoff_datetime'].dt.dayofweek\n",
    "        df['dropoff_day_of_month'] = df['tpep_dropoff_datetime'].dt.day\n",
    "        df['dropoff_month'] = df['tpep_dropoff_datetime'].dt.month\n",
    "\n",
    "        print(\"Feature columns extracted successfully.\")\n",
    "        return df\n",
    "\n",
    "    # Metodo para calcular o tempo de duração da viagem\n",
    "    def calculate_trip_duration(self, df):\n",
    "        \"\"\"Calculates the duration of the trip in minutes.\"\"\"\n",
    "\n",
    "        # Ensure the dataset has the necessary distance column\n",
    "        if 'tpep_dropoff_dateime' and 'tpep_pickup_datetime' not in df.columns:\n",
    "            raise ValueError(\"tpep time stamps collums are missing from the dataset.\")\n",
    "\n",
    "        # Compute trip duration in minutes\n",
    "        df['trip_duration_min'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "        print(\"Trip duration calculated successfully.\")\n",
    "        return df\n",
    "\n",
    "    # Metodo para calcular a velocidade média da viagem\n",
    "    def calculate_average_speed(self, df):\n",
    "        \"\"\"Calculates the average speed of the trip in km/h.\"\"\"\n",
    "\n",
    "        # Ensure the dataset has the necessary distance column\n",
    "        if 'trip_distance'  not in df.columns:\n",
    "            raise ValueError(\"Column 'trip_distance' is missing from the dataset.\")\n",
    "\n",
    "        # Avoid division by zero for extremely short trips\n",
    "        df['average_speed_mph'] = df['trip_distance'] / (df['trip_duration_min'] / 60)\n",
    "        df['average_speed_mph'].fillna(0, inplace=True)  # Replace NaN values with 0\n",
    "\n",
    "        print(\"Average speed calculated successfully.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def drop_columns(self, df):\n",
    "        \"\"\"Drops unnecessary or problematic features from the dataset\"\"\"\n",
    "        df.drop(columns=['tpep_pickup_datetime'], inplace=True)\n",
    "        df.drop(columns=['tpep_dropoff_datetime'], inplace=True)\n",
    "        return df\n",
    "\n"
   ],
   "id": "9c492bb0210fa918",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:31:34.257521Z",
     "start_time": "2025-03-08T15:31:34.238904Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": "# Data Preprocessing\n",
   "id": "517c5346e11c944c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Cleaning\n",
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    Class for cleaning operations.\n",
    "\n",
    "    Methods:\n",
    "        remove_duplicates(): Remove duplicate rows from the dataset.\n",
    "        handle_missing_values(strategy='mean'): Handle missing values using the specified strategy.\n",
    "        remove_outliers(threshold=3): Remove outliers from the dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, data_loader):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessing class with a DataLoader object.\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"\n",
    "        Remove duplicate rows from the train dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if data and labels are not None\n",
    "            if self.data_loader.data_train is None:\n",
    "                raise ValueError(\"Data has not been loaded yet.\")\n",
    "            if self.data_loader.labels_train is None:\n",
    "                raise ValueError(\"Labels have not been loaded yet.\")\n",
    "\n",
    "            # Remove duplicate rows from training data (do not apply to test data)\n",
    "            self.data_loader.data_train.drop_duplicates(inplace=True)\n",
    "            self.data_loader.labels_train = self.data_loader.labels_train[self.data_loader.data_train.index]\n",
    "\n",
    "            print(\"Duplicate rows removed from training data.\")\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(\"Error:\", ve)\n",
    "\n",
    "    def handle_missing_values(self, strategy='drop'):\n",
    "        \"\"\"\n",
    "        Handle missing values using the specified strategy.\n",
    "\n",
    "        Parameters:\n",
    "            strategy (str): The strategy to handle missing values ('mean', 'median', 'most_frequent', or a constant value).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if data is not None\n",
    "            if self.data_loader.data_train is None or self.data_loader.data_test is None :\n",
    "                raise ValueError(\"Data has not been loaded yet.\")\n",
    "\n",
    "            # Check if there are missing values\n",
    "            if self.data_loader.data_train.isnull().sum().sum() == 0 and self.data_loader.data_test.isnull().sum().sum() == 0:\n",
    "                print(\"No missing values found in the data.\")\n",
    "                return\n",
    "\n",
    "            # Handle missing values based on the specified strategy\n",
    "            if strategy == 'mean':\n",
    "                self.data_loader.data_train.fillna(self.data_loader.data_train.mean(), inplace=True)\n",
    "                self.data_loader.data_test.fillna(self.data_loader.data_test.mean(), inplace=True)\n",
    "            elif strategy == 'median':\n",
    "                self.data_loader.data_train.fillna(self.data_loader.data_train.median(), inplace=True)\n",
    "                self.data_loader.data_test.fillna(self.data_loader.data_test.median(), inplace=True)\n",
    "            elif strategy == 'most_frequent':\n",
    "                self.data_loader.data_train.fillna(self.data_loader.data_train.mode().iloc[0], inplace=True)\n",
    "                self.data_loader.data_test.fillna(self.data_loader.data_test.mode().iloc[0], inplace=True)\n",
    "            elif strategy == 'fill_nan':\n",
    "                self.data_loader.data_train.fillna(strategy, inplace=True)\n",
    "                self.data_loader.data_test.fillna(strategy, inplace=True)\n",
    "            elif strategy == 'drop':\n",
    "                self.data_loader.data_train = self.data_loader.data_train.dropna(axis=0)\n",
    "                self.data_loader.labels_train = self.data_loader.labels_train[self.data_loader.data_train.index]\n",
    "                self.data_loader.data_test = self.data_loader.data_test.dropna(axis=0)\n",
    "                self.data_loader.labels_test = self.data_loader.labels_test[self.data_loader.data_test.index]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid strategy.\")\n",
    "            print(\"Missing values handled using strategy:\", strategy)\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(\"Error:\", ve)\n",
    "\n",
    "    def _detect_outliers(self, threshold=4):\n",
    "        \"\"\"\n",
    "        Detect outliers in numerical features using z-score method.\n",
    "\n",
    "        Parameters:\n",
    "            threshold (float): The threshold value for determining outliers.\n",
    "\n",
    "        Returns:\n",
    "            outliers (DataFrame): DataFrame containing the outliers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if test data is not None\n",
    "            if self.data_loader.data_train is None:\n",
    "                raise ValueError(\"Data has not been loaded yet.\")\n",
    "\n",
    "            # Identify numerical features\n",
    "            numerical_features = self.data_loader.data_train.select_dtypes(include=['number'])\n",
    "\n",
    "            # Calculate z-scores for numerical features\n",
    "            z_scores = (numerical_features - numerical_features.mean()) / numerical_features.std()\n",
    "\n",
    "            # Find outliers based on threshold\n",
    "            outliers = self.data_loader.data_train[(z_scores.abs() > threshold).any(axis=1)]\n",
    "\n",
    "            return outliers\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(\"Error:\", ve)\n",
    "\n",
    "    def remove_outliers(self, threshold=2):\n",
    "        \"\"\"\n",
    "        Remove outliers from the dataset using z-score method.\n",
    "\n",
    "        Parameters:\n",
    "            threshold (float): The threshold value for determining outliers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if data_loader.data is not None\n",
    "            if self.data_loader.data_train is None:\n",
    "                raise ValueError(\"Data has not been loaded yet.\")\n",
    "\n",
    "            # Detect outliers\n",
    "            outliers = self._detect_outliers(threshold)\n",
    "\n",
    "            # Remove outliers from the dataset\n",
    "            self.data_loader.data_train = self.data_loader.data_train.drop(outliers.index)\n",
    "            self.data_loader.labels_train = self.data_loader.labels_train[self.data_loader.data_train.index]\n",
    "\n",
    "            print(\"Outliers removed from the dataset.\")\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(\"Error:\", ve)"
   ],
   "id": "a6f1c704b74f78da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:55:52.485349Z",
     "start_time": "2025-03-09T16:55:52.053998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing implementation\n",
    "\n",
    "# as tabelas vão de \"\\\\2019\\\\2019-01.sqlite\" a \"\\\\2019\\\\2019-12.sqlite\"\n",
    "\n",
    "sql_loader = KagglehubSQLiteLoader(\"dhruvildave/new-york-city-taxi-trips-2019\", \"\\\\2019\\\\2019-01.sqlite\")\n",
    "df = sql_loader.get_table_dataframe_nrows(\"tripdata\", 10)\n",
    "\n",
    "data_manipulator = TripDataManipulator(df, target=\"fare_amount\")\n",
    "data_manipulator.data_train\n"
   ],
   "id": "b95fe1a72a90c6e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.10)\n",
      "Dataset downloaded Successfully \n",
      "Dataset saved on:  C:\\Users\\usoda\\.cache\\kagglehub\\datasets\\dhruvildave\\new-york-city-taxi-trips-2019\\versions\\4\n",
      "Feature columns extracted successfully.\n",
      "Trip duration calculated successfully.\n",
      "Average speed calculated successfully.\n",
      "Data divided successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usoda\\AppData\\Local\\Temp\\ipykernel_4944\\4028433919.py:176: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['average_speed_mph'].fillna(0, inplace=True)  # Replace NaN values with 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "9       1.0  2019-01-01 00:57:32   2019-01-01 01:09:32              2.0   \n",
       "5       2.0  2018-11-28 16:25:49   2018-11-28 16:28:26              5.0   \n",
       "0       1.0  2019-01-01 00:46:40   2019-01-01 00:53:20              1.0   \n",
       "4       2.0  2018-11-28 15:56:57   2018-11-28 15:58:33              5.0   \n",
       "1       1.0  2019-01-01 00:59:47   2019-01-01 01:18:59              1.0   \n",
       "8       1.0  2019-01-01 00:32:01   2019-01-01 00:45:39              1.0   \n",
       "6       2.0  2018-11-28 16:29:37   2018-11-28 16:33:43              5.0   \n",
       "7       1.0  2019-01-01 00:21:28   2019-01-01 00:28:37              1.0   \n",
       "\n",
       "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
       "9            2.1         1.0                  N         141.0         234.0   \n",
       "5            0.0         1.0                  N         193.0         193.0   \n",
       "0            1.5         1.0                  N         151.0         239.0   \n",
       "4            0.0         2.0                  N         193.0         193.0   \n",
       "1            2.6         1.0                  N         239.0         246.0   \n",
       "8            3.7         1.0                  N         229.0           7.0   \n",
       "6            0.0         2.0                  N         193.0         193.0   \n",
       "7            1.3         1.0                  N         163.0         229.0   \n",
       "\n",
       "   payment_type  ...  pickup_hour  pickup_day_of_week  pickup_day_of_month  \\\n",
       "9           1.0  ...            0                   1                    1   \n",
       "5           2.0  ...           16                   2                   28   \n",
       "0           1.0  ...            0                   1                    1   \n",
       "4           2.0  ...           15                   2                   28   \n",
       "1           1.0  ...            0                   1                    1   \n",
       "8           1.0  ...            0                   1                    1   \n",
       "6           2.0  ...           16                   2                   28   \n",
       "7           1.0  ...            0                   1                    1   \n",
       "\n",
       "   pickup_month  dropoff_hour  dropoff_day_of_week dropoff_day_of_month  \\\n",
       "9             1             1                    1                    1   \n",
       "5            11            16                    2                   28   \n",
       "0             1             0                    1                    1   \n",
       "4            11            15                    2                   28   \n",
       "1             1             1                    1                    1   \n",
       "8             1             0                    1                    1   \n",
       "6            11            16                    2                   28   \n",
       "7             1             0                    1                    1   \n",
       "\n",
       "   dropoff_month  trip_duration_min  average_speed_mph  \n",
       "9              1          12.000000          10.500000  \n",
       "5             11           2.616667           0.000000  \n",
       "0              1           6.666667          13.500000  \n",
       "4             11           1.600000           0.000000  \n",
       "1              1          19.200000           8.125000  \n",
       "8              1          13.633333          16.283619  \n",
       "6             11           4.100000           0.000000  \n",
       "7              1           7.150000          10.909091  \n",
       "\n",
       "[8 rows x 27 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>...</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_day_of_week</th>\n",
       "      <th>pickup_day_of_month</th>\n",
       "      <th>pickup_month</th>\n",
       "      <th>dropoff_hour</th>\n",
       "      <th>dropoff_day_of_week</th>\n",
       "      <th>dropoff_day_of_month</th>\n",
       "      <th>dropoff_month</th>\n",
       "      <th>trip_duration_min</th>\n",
       "      <th>average_speed_mph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:57:32</td>\n",
       "      <td>2019-01-01 01:09:32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>141.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 16:25:49</td>\n",
       "      <td>2018-11-28 16:28:26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:46:40</td>\n",
       "      <td>2019-01-01 00:53:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>151.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>13.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 15:56:57</td>\n",
       "      <td>2018-11-28 15:58:33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:59:47</td>\n",
       "      <td>2019-01-01 01:18:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>8.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:32:01</td>\n",
       "      <td>2019-01-01 00:45:39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>229.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.633333</td>\n",
       "      <td>16.283619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 16:29:37</td>\n",
       "      <td>2018-11-28 16:33:43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:21:28</td>\n",
       "      <td>2019-01-01 00:28:37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>163.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.150000</td>\n",
       "      <td>10.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate new dataset for next phases\n",
    "\n",
    "# Load all 12 SQLite databases with data from all months\n",
    "# Random sample those databases to get a general sample with data from all of them\n",
    "# Join all the samples in a single dataframe\n",
    "# Apply Manipulation\n",
    "\n",
    "# Apply Preprocessing\n",
    "\n",
    "# Apply Clean up\n",
    "\n",
    "# Save it\n"
   ],
   "id": "8d37af92bb62f344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EDA (Exploratory Data Analysis)",
   "id": "d7795fd163155de3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#EDA",
   "id": "5bb3c479715ae2dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Data Visualization\n",
    "class DataFrameDisplayer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def plot_column(self, column: str, plot_type: str) -> None:\n",
    "        \"\"\"\n",
    "        This method plots a column data with one of the plot types\n",
    "        :param column: column name from dataframe\n",
    "        :param plot_type: should be in ['hist', 'violin', 'box', 'scatter', 'lines', 'bar']\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        plot_types = ['hist', 'violin', 'box', 'scatter', 'lines', 'bar']\n",
    "        if plot_type not in plot_types:\n",
    "            raise ValueError('plot_type must be in {}'.format(plot_types))\n",
    "\n",
    "        if column not in self.df.columns:\n",
    "            raise ValueError('column must be in {}'.format(column))\n",
    "\n",
    "        plt.figure(figsize = (12, 8))\n",
    "\n",
    "        if plot_type == 'hist':\n",
    "            sns.histplot(self.df[column], kde=True, bins=30)\n",
    "        elif plot_type == 'violin':\n",
    "            sns.violinplot(y=self.df[column])\n",
    "        elif plot_type == 'box':\n",
    "            sns.boxplot(y=self.df[column])\n",
    "        elif plot_type == 'scatter':\n",
    "            if self.df.shape[1] < 2:\n",
    "                raise ValueError(\"Scatter plots require at least two columns\")\n",
    "            sns.scatterplot(x=self.df.index, y=self.df[column])\n",
    "        elif plot_type == 'line':\n",
    "            sns.lineplot(x=self.df.index, y=self.df[column])\n",
    "        elif plot_type == 'bar':\n",
    "            sns.barplot(x=self.df.index, y=self.df[column])\n",
    "\n",
    "        plt.title(f'{plot_type.capitalize()} plot of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Values\")\n",
    "        plt.show()"
   ],
   "id": "a8fa60a8f27601be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Feature Analysis\n",
    "\n",
    "#Feature Generator"
   ],
   "id": "b2233c9451b4b1d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistical Analysis\n",
   "id": "5fff6173fa4344c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Hypothesis Testing",
   "id": "764a0d955e6144ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d5384ab505aec8da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
