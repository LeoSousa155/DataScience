{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Descrição do problema\n",
    "\n",
    "No âmbito da unidade curricular de Ciência de Dados, foi proposto o desenvolvimento de um modelo de machine learning utilizando o dataset \"New York City Taxi Trips\". O objetivo é construir um modelo capaz de prever o preço das viagens de táxi com base em diversas variáveis disponíveis, como distância da viagem, horário do dia, número de passageiros e possíveis condições de tráfego.\n",
    "\n",
    "# Objetivo\n",
    "O principal objetivo deste projeto é desenvolver um modelo que consiga estimar o valor das tarifas de táxi em Nova York. A tarefa será abordada sob duas perspectivas:\n",
    "\n",
    "1. Regressão: Criar um modelo que consiga prever com precisão o valor exato da tarifa para uma determinada viagem.\n",
    "2. Classificação: Reformular o problema para classificar as viagens em faixas de preços predefinidas:\n",
    "\n",
    "    - Classe 1: Viagens curtas e de baixo custo (`< $10`)\n",
    "\n",
    "    - Classe 2: Viagens de média distância e preço moderado (`$10 - $30`)\n",
    "\n",
    "    - Classe 3: Viagens longas com tarifas mais elevadas (`$30 - $60`)\n",
    "\n",
    "    - Classe 4: Tarifas premium (`> $60`)\n",
    "\n",
    "# Descrição dos Dados\n",
    "\n",
    "O dataset **New York City Yellow Taxi Trip Records** contém informações detalhadas sobre viagens de táxi na cidade de Nova York, incluindo dados sobre tempo, distância, localizações de embarque e desembarque, além de informações de pagamento.\n",
    "\n",
    "O dataset inclui as seguintes colunas:\n",
    "\n",
    "- **VendorID**: Código que identifica o provedor do sistema de processamento eletrônico de pagamentos (TPEP).\n",
    "  - 1 = Creative Mobile Technologies, LLC\n",
    "  - 2 = VeriFone Inc.\n",
    "\n",
    "- **tpep_pickup_datetime**: Data e hora em que o taxímetro foi ativado.\n",
    "- **tpep_dropoff_datetime**: Data e hora em que o taxímetro foi desativado.\n",
    "- **Passenger_count**: Número de passageiros no veículo (valor inserido pelo motorista).\n",
    "- **Trip_distance**: Distância percorrida na viagem (em milhas) reportada pelo taxímetro.\n",
    "- **PULocationID**: Código da zona TLC onde o taxímetro foi ativado (embarque).\n",
    "- **DOLocationID**: Código da zona TLC onde o taxímetro foi desativado (desembarque).\n",
    "- **RateCodeID**: Código da tarifa aplicada ao final da viagem.\n",
    "  - 1 = Tarifa padrão\n",
    "  - 2 = JFK\n",
    "  - 3 = Newark\n",
    "  - 4 = Nassau ou Westchester\n",
    "  - 5 = Tarifa negociada\n",
    "  - 6 = Viagem em grupo\n",
    "\n",
    "- **Store_and_fwd_flag**: Indica se o registro da viagem foi armazenado antes de ser enviado ao provedor, devido à falta de conexão do veículo com o servidor.\n",
    "  - Y = Viagem armazenada antes do envio\n",
    "  - N = Viagem enviada em tempo real\n",
    "\n",
    "- **Payment_type**: Código que indica a forma de pagamento utilizada pelo passageiro.\n",
    "  - 1 = Cartão de crédito\n",
    "  - 2 = Dinheiro\n",
    "  - 3 = Sem cobrança\n",
    "  - 4 = Disputa\n",
    "  - 5 = Desconhecido\n",
    "  - 6 = Viagem cancelada\n",
    "\n",
    "- **Fare_amount**: Valor da tarifa baseado no tempo e na distância percorrida.\n",
    "- **Extra**: Cobranças adicionais, como sobretaxas noturnas ou de horário de pico.\n",
    "- **MTA_tax**: Taxa de `$0,50` do MTA aplicada automaticamente com base na tarifa do taxímetro.\n",
    "- **Improvement_surcharge**: Taxa de melhoria de `$0,30` aplicada desde 2015.\n",
    "- **Tip_amount**: Valor da gorjeta (preenchido automaticamente para pagamentos com cartão; gorjetas em dinheiro não são registradas).\n",
    "- **Tolls_amount**: Valor total de pedágios pagos durante a viagem.\n",
    "- **Total_amount**: Valor total cobrado do passageiro (não inclui gorjetas pagas em dinheiro).\n"
   ],
   "id": "66a1dd6e3ef3af8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3 as sql\n",
    "import kagglehub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n"
   ],
   "id": "195c52c539317959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Analysis\n",
    "\n",
    "A análise de dados será realizada com a abordagem ELT (Extract, Load, Transform). Inicialmente, os dados serão extraídos do KaggleHub e uma amostra aleatória será utilizada para trabalhar com uma fração representativa do conjunto completo. Em seguida, os dados extraídos serão carregados em estruturas como DataFrames Pandas ou arrays NumPy, facilitando a manipulação e análise. Por fim, na fase de transformação, os dados serão limpos, normalizados e preparados para gerar métricas e insights. A amostragem aleatória permite reduzir a complexidade, permitindo uma análise eficiente sem processar todos os dados de uma vez."
   ],
   "id": "36ab584fc91a2de7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class KagglehubDatabaseLoader:\n",
    "    \"\"\"\n",
    "    A class to download and manage datasets from KaggleHub.\n",
    "\n",
    "    This class automates the process of downloading a dataset from KaggleHub\n",
    "    and provides a method to retrieve the local file path.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (str): The name of the dataset to be downloaded.\n",
    "        _path (str or None): The local directory path where the dataset is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str):\n",
    "        \"\"\"\n",
    "        Initializes the KagglehubDatabaseLoader class.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): The name of the dataset to be downloaded.\n",
    "        \"\"\"\n",
    "        self._dataset = dataset\n",
    "        self._path = None\n",
    "\n",
    "        self._download_dataset()\n",
    "\n",
    "\n",
    "    def _download_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Downloads the specified dataset from KaggleHub.\n",
    "\n",
    "        This method attempts to download the dataset and assigns the local\n",
    "        file path to the `_path` attribute. If an error occurs during the\n",
    "        download, an exception is caught and printed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._path = kagglehub.dataset_download(self._dataset)\n",
    "            print(\"Dataset downloaded Successfully \")\n",
    "            print(\"Dataset saved on: \", self._path)\n",
    "        except Exception as e:\n",
    "            print(\"Error downloading dataset:\", e)\n",
    "\n",
    "\n",
    "    def get_path(self) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns the local directory path of the downloaded dataset.\n",
    "\n",
    "        Returns:\n",
    "            str or None: The file path of the dataset if the download was\n",
    "                        successful, otherwise None.\n",
    "        \"\"\"\n",
    "        return self._path\n",
    "\n",
    "\n",
    "class KagglehubSQLiteLoader(KagglehubDatabaseLoader):\n",
    "    \"\"\"\n",
    "    A subclass of KagglehubDatabaseLoader to interact with an SQLite dataset.\n",
    "\n",
    "    This class adds methods to retrieve information about tables, numpy arrays and dataframes.\n",
    "    the SQLite dataset is downloaded from KaggleHub.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (str): The name of the dataset to be downloaded from KaggleHub.\n",
    "        _path (str or None): The local directory path where the dataset is saved.\n",
    "        _file_path (str or None): The complete path to the specific SQLite file within the dataset.\n",
    "        _conn: The SQLite connection object used to interact with the database.\n",
    "        _cursor: The cursor object for executing SQL queries on the database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str, file: str):\n",
    "        \"\"\"\n",
    "        Initializes the KagglehubSQLLoader class by calling the superclass constructor\n",
    "        to download the dataset and setting up the SQLite connection.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): The dataset name to be downloaded.\n",
    "            file (str): The name of the SQLite file in the downloaded dataset to be opened.\n",
    "        \"\"\"\n",
    "        super().__init__(dataset)\n",
    "        self._file_path = self._path + file\n",
    "        self._conn = sql.connect(self._file_path)\n",
    "        self._cursor = self._conn.cursor()\n",
    "\n",
    "\n",
    "    def get_table_names(self) -> list | None:\n",
    "        \"\"\"\n",
    "        Retrieves all table names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of table names if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            table_name_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "            return self._cursor.execute(table_name_query).fetchall()\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table names:\", e)\n",
    "\n",
    "\n",
    "    def get_column_names(self, table: str) -> list | None:\n",
    "        \"\"\"\n",
    "        Retrieves the column names for a given table in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table for which to get column names.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of column names if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "            columns = [col[1] for col in self._cursor.fetchall()]\n",
    "            return columns\n",
    "        except Exception as e:\n",
    "            print(\"Error getting column names:\", e)\n",
    "\n",
    "\n",
    "    def get_table_row_count(self, table: str) -> int | None:\n",
    "        \"\"\"\n",
    "        Retrieves the row count for a given table.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table for which to get the row count.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of rows in the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT COUNT(*) FROM {table};\")\n",
    "            return self._cursor.fetchone()[0]\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table row count:\", e)\n",
    "\n",
    "\n",
    "    def get_table_data(self, table: str) -> np.ndarray | None:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a given table as a NumPy array.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array containing the data from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT * FROM {table}\")\n",
    "            data = self._cursor.fetchall()\n",
    "            column_names = self.get_column_names(table)\n",
    "            df = pd.DataFrame(data, columns=column_names)\n",
    "            return df.to_numpy()\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table data array:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe(self, table: str) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a given table as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the data from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT * from {table}\")\n",
    "            data = self._cursor.fetchall()\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table dataframe:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe_nrows(self, table: str, n: int) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves the first `n` rows of a given table as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "            n (int): The number of rows to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the first `n` rows from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query = \"SELECT * from tripdata\"\n",
    "            data = self._cursor.execute(query).fetchmany(n)\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting the first {n} rows from table:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe_random_sample(self, table: str, percentage: float = 0.05) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves a random sample of rows from the specified table based on a given percentage.\n",
    "\n",
    "        This method selects a random subset of the rows from the given table. The percentage of\n",
    "        rows to sample is specified by the `percentage` argument. The sampling is done using\n",
    "        the SQLite `RANDOM()` function, which provides a pseudo-random selection of rows.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table from which to retrieve the random sample.\n",
    "            percentage (float, optional): The fraction of rows to sample, between 0.0 and 1.0.\n",
    "                                          Defaults to 0.05 (5%).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: A Pandas DataFrame containing the random sample of rows.\n",
    "                                  Returns an empty DataFrame if no data is found, or None if an error occurs.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the `percentage` is not between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not 0.0 <= percentage <= 1.0:\n",
    "                raise ValueError(\"Percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "            size = self.get_table_row_count(table)\n",
    "            threshold = int(round(percentage * size))\n",
    "\n",
    "            query = \"SELECT * FROM tripdata WHERE ABS(RANDOM()) % ? < ?\"\n",
    "\n",
    "            self._cursor.execute(query, (size, threshold))\n",
    "            data = self._cursor.fetchall()\n",
    "\n",
    "            if not data:\n",
    "                return pd.DataFrame(columns=self.get_column_names(table))\n",
    "\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting random sample from table:\", e)"
   ],
   "id": "aed381c472c84667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T16:07:15.470780Z",
     "start_time": "2025-03-08T16:07:15.359928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataAnalizer:\n",
    "    \"\"\"\n",
    "    A class to load and split a dataset into training and testing sets.\n",
    "\n",
    "    This class is designed to handle the process of dividing a DataFrame into\n",
    "    training and testing sets for machine learning tasks. It takes in a DataFrame,\n",
    "    a target column, and splits the data into features and labels, which are then\n",
    "    separated into training and testing datasets.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        target (str): The name of the target column in the dataset.\n",
    "        test_size (float): The proportion of the dataset to include in the test split (default is 0.2).\n",
    "        random_state (int or None): The random seed for reproducibility of the split (default is None).\n",
    "        data_train (pd.DataFrame): The feature data for the training set.\n",
    "        labels_train (pd.Series): The labels for the training set.\n",
    "        data_test (pd.DataFrame): The feature data for the testing set.\n",
    "        labels_test (pd.Series): The labels for the testing set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, target: str,  test_size= 0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader instance and splits the dataset into training and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset to be split.\n",
    "            target (str): The name of the target column in the dataset.\n",
    "            test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "            random_state (int or None, optional): The random seed used for shuffling the data. Default is None.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.data_train = None\n",
    "        self.labels_train = None\n",
    "        self.data_test = None\n",
    "        self.labels_test = None\n",
    "\n",
    "        #divide data in train and test sets\n",
    "        self._divide_data()\n",
    "\n",
    "\n",
    "    def _divide_data(self):\n",
    "        \"\"\"\n",
    "        Splits the DataFrame into features (X) and labels (y), and divides them into training and testing sets.\n",
    "\n",
    "        The method uses `train_test_split()` from scikit-learn to randomly split the data into\n",
    "        training and test sets based on the specified `test_size` and `random_state`.\n",
    "\n",
    "        The method sets the following attributes:\n",
    "            - data_train: Feature data for training.\n",
    "            - labels_train: Labels for training.\n",
    "            - data_test: Feature data for testing.\n",
    "            - labels_test: Labels for testing.\n",
    "        \"\"\"\n",
    "        x = self.df.drop(columns=[self.target])\n",
    "        y = self.df[self.target]\n",
    "        x_train, y_train, x_test, y_test = train_test_split(\n",
    "            x, y, test_size=self.test_size,random_state=self.random_state\n",
    "        )\n",
    "        self.data_train  = x_train\n",
    "        self.labels_train = y_train\n",
    "        self.data_test    = x_test\n",
    "        self.labels_test  = y_test\n",
    "        print(\"Data divided successfully.\")\n",
    "\n",
    "\n",
    "    def _drop_columns(self, cols):\n",
    "        \"\"\"\n",
    "        Drops a specified feature columns from both the training and testing datasets.\n",
    "\n",
    "        Args:\n",
    "            cols (str): The list of the columns to be dropped from both the training and testing datasets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_train.drop(cols, axis=1, inplace=True)\n",
    "            self.data_test.drop(cols, axis=1, inplace=True)\n",
    "            print(f\"Feature columns {cols} dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping columns {cols}:\", e)\n",
    "\n",
    "\n",
    "\n",
    "class TripDataAnalizer(DataAnalizer):\n",
    "    def __init__(self, df: pd.DataFrame, target: str, test_size=0.2, random_state=None):\n",
    "        super().__init__(df, target, test_size, random_state)\n",
    "\n",
    "\n",
    "    # Metodo para separar \"tpep_pickup_datetime\" e \"tpep_dropoff_datetime\" em várias features\n",
    "    def extract_datetime_features(self):\n",
    "        # Ensure datetime columns are in the correct format\n",
    "        self.df['tpep_pickup_datetime'] = pd.to_datetime(self.df['tpep_pickup_datetime'])\n",
    "        self.df['tpep_dropoff_datetime'] = pd.to_datetime(self.df['tpep_dropoff_datetime'])\n",
    "\n",
    "        print(self.df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']].head())\n",
    "\n",
    "        # Extract features from pickup time\n",
    "        self.df['pickup_hour'] = self.df['tpep_pickup_datetime'].dt.hour\n",
    "        self.df['pickup_day_of_week'] = self.df['tpep_pickup_datetime'].dt.dayofweek\n",
    "        self.df['pickup_month'] = self.df['tpep_pickup_datetime'].dt.month\n",
    "\n",
    "        # Extract features from dropoff time\n",
    "        self.df['dropoff_hour'] = self.df['tpep_dropoff_datetime'].dt.hour\n",
    "        self.df['dropoff_day_of_week'] = self.df['tpep_dropoff_datetime'].dt.dayofweek\n",
    "        self.df['dropoff_month'] = self.df['tpep_dropoff_datetime'].dt.month\n",
    "\n",
    "        # Compute trip duration in minutes\n",
    "        self.df['trip_duration_min'] = (self.df['tpep_dropoff_datetime'] - self.df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "        print(\"Feature columns extracted successfully.\")\n",
    "        return self.df\n",
    "\n",
    "    # Metodo para calcular o tempo de duração da viagem\n",
    "    def calculate_trip_duration(self):\n",
    "        \"\"\"Calculates the duration of the trip in minutes.\"\"\"\n",
    "\n",
    "        # Compute trip duration in minutes\n",
    "        self.df['trip_duration_min'] = (self.df['tpep_dropoff_datetime'] - self.df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "        print(\"Trip duration calculated successfully.\")\n",
    "        return self.df\n",
    "\n",
    "    # Metodo para calcular a velocidade média da viagem\n",
    "    def calculate_average_speed(self):\n",
    "        \"\"\"Calculates the average speed of the trip in km/h.\"\"\"\n",
    "\n",
    "        # Ensure the dataset has the necessary distance column\n",
    "        if 'trip_distance' not in self.df.columns:\n",
    "            raise ValueError(\"Column 'trip_distance' is missing from the dataset.\")\n",
    "\n",
    "        # Avoid division by zero for extremely short trips\n",
    "        self.df['average_speed_kmh'] = self.df['trip_distance'] / (self.df['trip_duration_min'] / 60)\n",
    "        self.df['average_speed_kmh'].fillna(0, inplace=True)  # Replace NaN values with 0\n",
    "\n",
    "        print(\"Average speed calculated successfully.\")\n",
    "        return self.df\n",
    "\n",
    "\n"
   ],
   "id": "9c492bb0210fa918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data divided successfully.\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance\n",
      "0  2024-03-01 10:30:00   2024-03-01 10:50:00            5.2\n",
      "1  2024-03-01 11:15:00   2024-03-01 11:45:00            8.5\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime\n",
      "0  2024-03-01 10:30:00   2024-03-01 10:50:00\n",
      "1  2024-03-01 11:15:00   2024-03-01 11:45:00\n",
      "Feature columns extracted successfully.\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance  pickup_hour  \\\n",
      "0  2024-03-01 10:30:00   2024-03-01 10:50:00            5.2           10   \n",
      "1  2024-03-01 11:15:00   2024-03-01 11:45:00            8.5           11   \n",
      "\n",
      "   pickup_day_of_week  pickup_month  dropoff_hour  dropoff_day_of_week  \\\n",
      "0                   4             3            10                    4   \n",
      "1                   4             3            11                    4   \n",
      "\n",
      "   dropoff_month  trip_duration_min  \n",
      "0              3               20.0  \n",
      "1              3               30.0  \n",
      "Average speed calculated successfully.\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance  pickup_hour  \\\n",
      "0  2024-03-01 10:30:00   2024-03-01 10:50:00            5.2           10   \n",
      "1  2024-03-01 11:15:00   2024-03-01 11:45:00            8.5           11   \n",
      "\n",
      "   pickup_day_of_week  pickup_month  dropoff_hour  dropoff_day_of_week  \\\n",
      "0                   4             3            10                    4   \n",
      "1                   4             3            11                    4   \n",
      "\n",
      "   dropoff_month  trip_duration_min  average_speed_kmh  \n",
      "0              3               20.0               15.6  \n",
      "1              3               30.0               17.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_20988\\1604569215.py:136: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.df['average_speed_kmh'].fillna(0, inplace=True)  # Replace NaN values with 0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:31:34.257521Z",
     "start_time": "2025-03-08T15:31:34.238904Z"
    }
   },
   "cell_type": "code",
   "source": "# Data Preprocessing\n",
   "id": "517c5346e11c944c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Data Cleaning",
   "id": "a6f1c704b74f78da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:30:49.862075Z",
     "start_time": "2025-03-08T15:30:49.153793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing implementation\n",
    "\n",
    "# as tabelas vão de \"\\\\2019\\\\2019-01.sqlite\" a \"\\\\2019\\\\2019-12.sqlite\"\n",
    "\n",
    "sql_loader = KagglehubSQLiteLoader(\"dhruvildave/new-york-city-taxi-trips-2019\", \"\\\\2019\\\\2019-01.sqlite\")\n",
    "df = sql_loader.get_table_dataframe_nrows(\"tripdata\", 10)\n",
    "df\n"
   ],
   "id": "b95fe1a72a90c6e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded Successfully \n",
      "Dataset saved on:  C:\\Users\\bruno\\.cache\\kagglehub\\datasets\\dhruvildave\\new-york-city-taxi-trips-2019\\versions\\4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   vendorid        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
       "0       1.0  2019-01-01 00:46:40.000000  2019-01-01 00:53:20.000000   \n",
       "1       1.0  2019-01-01 00:59:47.000000  2019-01-01 01:18:59.000000   \n",
       "2       2.0  2018-12-21 13:48:30.000000  2018-12-21 13:52:40.000000   \n",
       "3       2.0  2018-11-28 15:52:25.000000  2018-11-28 15:55:45.000000   \n",
       "4       2.0  2018-11-28 15:56:57.000000  2018-11-28 15:58:33.000000   \n",
       "5       2.0  2018-11-28 16:25:49.000000  2018-11-28 16:28:26.000000   \n",
       "6       2.0  2018-11-28 16:29:37.000000  2018-11-28 16:33:43.000000   \n",
       "7       1.0  2019-01-01 00:21:28.000000  2019-01-01 00:28:37.000000   \n",
       "8       1.0  2019-01-01 00:32:01.000000  2019-01-01 00:45:39.000000   \n",
       "9       1.0  2019-01-01 00:57:32.000000  2019-01-01 01:09:32.000000   \n",
       "\n",
       "   passenger_count  trip_distance  ratecodeid store_and_fwd_flag  \\\n",
       "0              1.0            1.5         1.0                  N   \n",
       "1              1.0            2.6         1.0                  N   \n",
       "2              3.0            0.0         1.0                  N   \n",
       "3              5.0            0.0         1.0                  N   \n",
       "4              5.0            0.0         2.0                  N   \n",
       "5              5.0            0.0         1.0                  N   \n",
       "6              5.0            0.0         2.0                  N   \n",
       "7              1.0            1.3         1.0                  N   \n",
       "8              1.0            3.7         1.0                  N   \n",
       "9              2.0            2.1         1.0                  N   \n",
       "\n",
       "   pulocationid  dolocationid  payment_type  fare_amount  extra  mta_tax  \\\n",
       "0         151.0         239.0           1.0          7.0    0.5      0.5   \n",
       "1         239.0         246.0           1.0         14.0    0.5      0.5   \n",
       "2         236.0         236.0           1.0          4.5    0.5      0.5   \n",
       "3         193.0         193.0           2.0          3.5    0.5      0.5   \n",
       "4         193.0         193.0           2.0         52.0    0.0      0.5   \n",
       "5         193.0         193.0           2.0          3.5    0.5      0.5   \n",
       "6         193.0         193.0           2.0         52.0    0.0      0.5   \n",
       "7         163.0         229.0           1.0          6.5    0.5      0.5   \n",
       "8         229.0           7.0           1.0         13.5    0.5      0.5   \n",
       "9         141.0         234.0           1.0         10.0    0.5      0.5   \n",
       "\n",
       "   tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
       "0        1.65          0.00                    0.3          9.95   \n",
       "1        1.00          0.00                    0.3         16.30   \n",
       "2        0.00          0.00                    0.3          5.80   \n",
       "3        0.00          0.00                    0.3          7.55   \n",
       "4        0.00          0.00                    0.3         55.55   \n",
       "5        0.00          5.76                    0.3         13.31   \n",
       "6        0.00          0.00                    0.3         55.55   \n",
       "7        1.25          0.00                    0.3          9.05   \n",
       "8        3.70          0.00                    0.3         18.50   \n",
       "9        1.70          0.00                    0.3         13.00   \n",
       "\n",
       "  congestion_surcharge  \n",
       "0                 None  \n",
       "1                 None  \n",
       "2                 None  \n",
       "3                 None  \n",
       "4                 None  \n",
       "5                 None  \n",
       "6                 None  \n",
       "7                 None  \n",
       "8                 None  \n",
       "9                 None  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:46:40.000000</td>\n",
       "      <td>2019-01-01 00:53:20.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>151.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.95</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:59:47.000000</td>\n",
       "      <td>2019-01-01 01:18:59.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-21 13:48:30.000000</td>\n",
       "      <td>2018-12-21 13:52:40.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 15:52:25.000000</td>\n",
       "      <td>2018-11-28 15:55:45.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 15:56:57.000000</td>\n",
       "      <td>2018-11-28 15:58:33.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55.55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 16:25:49.000000</td>\n",
       "      <td>2018-11-28 16:28:26.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 16:29:37.000000</td>\n",
       "      <td>2018-11-28 16:33:43.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55.55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:21:28.000000</td>\n",
       "      <td>2019-01-01 00:28:37.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>163.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.05</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:32:01.000000</td>\n",
       "      <td>2019-01-01 00:45:39.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>229.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>18.50</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:57:32.000000</td>\n",
       "      <td>2019-01-01 01:09:32.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>141.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EDA (Exploratory Data Analysis)",
   "id": "d7795fd163155de3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5bb3c479715ae2dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:03:31.588585Z",
     "start_time": "2025-03-08T15:03:30.331287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3 as sql\n",
    "import kagglehub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n"
   ],
   "id": "940bc2c0cdc7797c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Analysis\n",
    "\n",
    "A análise de dados será realizada com a abordagem ELT (Extract, Load, Transform). Inicialmente, os dados serão extraídos do KaggleHub e uma amostra aleatória será utilizada para trabalhar com uma fração representativa do conjunto completo. Em seguida, os dados extraídos serão carregados em estruturas como DataFrames Pandas ou arrays NumPy, facilitando a manipulação e análise. Por fim, na fase de transformação, os dados serão limpos, normalizados e preparados para gerar métricas e insights. A amostragem aleatória permite reduzir a complexidade, permitindo uma análise eficiente sem processar todos os dados de uma vez."
   ],
   "id": "407e35c7a84bb1e3"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-08T15:03:35.681593Z",
     "start_time": "2025-03-08T15:03:35.656944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KagglehubDatabaseLoader:\n",
    "    \"\"\"\n",
    "    A class to download and manage datasets from KaggleHub.\n",
    "\n",
    "    This class automates the process of downloading a dataset from KaggleHub\n",
    "    and provides a method to retrieve the local file path.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (str): The name of the dataset to be downloaded.\n",
    "        _path (str or None): The local directory path where the dataset is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str):\n",
    "        \"\"\"\n",
    "        Initializes the KagglehubDatabaseLoader class.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): The name of the dataset to be downloaded.\n",
    "        \"\"\"\n",
    "        self._dataset = dataset\n",
    "        self._path = None\n",
    "\n",
    "        self._download_dataset()\n",
    "\n",
    "\n",
    "    def _download_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Downloads the specified dataset from KaggleHub.\n",
    "\n",
    "        This method attempts to download the dataset and assigns the local\n",
    "        file path to the `_path` attribute. If an error occurs during the\n",
    "        download, an exception is caught and printed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._path = kagglehub.dataset_download(self._dataset)\n",
    "            print(\"Dataset downloaded Successfully \")\n",
    "            print(\"Dataset saved on: \", self._path)\n",
    "        except Exception as e:\n",
    "            print(\"Error downloading dataset:\", e)\n",
    "\n",
    "\n",
    "    def get_path(self) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns the local directory path of the downloaded dataset.\n",
    "\n",
    "        Returns:\n",
    "            str or None: The file path of the dataset if the download was\n",
    "                        successful, otherwise None.\n",
    "        \"\"\"\n",
    "        return self._path\n",
    "\n",
    "\n",
    "class KagglehubSQLiteLoader(KagglehubDatabaseLoader):\n",
    "    \"\"\"\n",
    "    A subclass of KagglehubDatabaseLoader to interact with an SQLite dataset.\n",
    "\n",
    "    This class adds methods to retrieve information about tables, numpy arrays and dataframes.\n",
    "    the SQLite dataset is downloaded from KaggleHub.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (str): The name of the dataset to be downloaded from KaggleHub.\n",
    "        _path (str or None): The local directory path where the dataset is saved.\n",
    "        _file_path (str or None): The complete path to the specific SQLite file within the dataset.\n",
    "        _conn: The SQLite connection object used to interact with the database.\n",
    "        _cursor: The cursor object for executing SQL queries on the database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str, file: str):\n",
    "        \"\"\"\n",
    "        Initializes the KagglehubSQLLoader class by calling the superclass constructor\n",
    "        to download the dataset and setting up the SQLite connection.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): The dataset name to be downloaded.\n",
    "            file (str): The name of the SQLite file in the downloaded dataset to be opened.\n",
    "        \"\"\"\n",
    "        super().__init__(dataset)\n",
    "        self._file_path = self._path + file\n",
    "        self._conn = sql.connect(self._file_path)\n",
    "        self._cursor = self._conn.cursor()\n",
    "\n",
    "\n",
    "    def get_table_names(self) -> list | None:\n",
    "        \"\"\"\n",
    "        Retrieves all table names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of table names if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            table_name_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "            return self._cursor.execute(table_name_query).fetchall()\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table names:\", e)\n",
    "\n",
    "\n",
    "    def get_column_names(self, table: str) -> list | None:\n",
    "        \"\"\"\n",
    "        Retrieves the column names for a given table in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table for which to get column names.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of column names if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "            columns = [col[1] for col in self._cursor.fetchall()]\n",
    "            return columns\n",
    "        except Exception as e:\n",
    "            print(\"Error getting column names:\", e)\n",
    "\n",
    "\n",
    "    def get_table_row_count(self, table: str) -> int | None:\n",
    "        \"\"\"\n",
    "        Retrieves the row count for a given table.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table for which to get the row count.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of rows in the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT COUNT(*) FROM {table};\")\n",
    "            return self._cursor.fetchone()[0]\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table row count:\", e)\n",
    "\n",
    "\n",
    "    def get_table_data(self, table: str) -> np.ndarray | None:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a given table as a NumPy array.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array containing the data from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT * FROM {table}\")\n",
    "            data = self._cursor.fetchall()\n",
    "            column_names = self.get_column_names(table)\n",
    "            df = pd.DataFrame(data, columns=column_names)\n",
    "            return df.to_numpy()\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table data array:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe(self, table: str) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a given table as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the data from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._cursor.execute(f\"SELECT * from {table}\")\n",
    "            data = self._cursor.fetchall()\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"Error getting table dataframe:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe_nrows(self, table: str, n: int) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves the first `n` rows of a given table as a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table to retrieve data from.\n",
    "            n (int): The number of rows to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the first `n` rows from the table if successful, None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query = \"SELECT * from tripdata\"\n",
    "            data = self._cursor.execute(query).fetchmany(n)\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting the first {n} rows from table:\", e)\n",
    "\n",
    "\n",
    "    def get_table_dataframe_random_sample(self, table: str, percentage: float = 0.05) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Retrieves a random sample of rows from the specified table based on a given percentage.\n",
    "\n",
    "        This method selects a random subset of the rows from the given table. The percentage of\n",
    "        rows to sample is specified by the `percentage` argument. The sampling is done using\n",
    "        the SQLite `RANDOM()` function, which provides a pseudo-random selection of rows.\n",
    "\n",
    "        Args:\n",
    "            table (str): The name of the table from which to retrieve the random sample.\n",
    "            percentage (float, optional): The fraction of rows to sample, between 0.0 and 1.0.\n",
    "                                          Defaults to 0.05 (5%).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: A Pandas DataFrame containing the random sample of rows.\n",
    "                                  Returns an empty DataFrame if no data is found, or None if an error occurs.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the `percentage` is not between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not 0.0 <= percentage <= 1.0:\n",
    "                raise ValueError(\"Percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "            size = self.get_table_row_count(table)\n",
    "            threshold = int(round(percentage * size))\n",
    "\n",
    "            query = \"SELECT * FROM tripdata WHERE ABS(RANDOM()) % ? < ?\"\n",
    "\n",
    "            self._cursor.execute(query, (size, threshold))\n",
    "            data = self._cursor.fetchall()\n",
    "\n",
    "            if not data:\n",
    "                return pd.DataFrame(columns=self.get_column_names(table))\n",
    "\n",
    "            df = pd.DataFrame.from_records(data)\n",
    "            df.columns = self.get_column_names(table)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting random sample from table:\", e)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T14:39:48.562960800Z",
     "start_time": "2025-03-05T23:53:39.414512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataAnalizer:\n",
    "    \"\"\"\n",
    "    A class to load and split a dataset into training and testing sets.\n",
    "\n",
    "    This class is designed to handle the process of dividing a DataFrame into\n",
    "    training and testing sets for machine learning tasks. It takes in a DataFrame,\n",
    "    a target column, and splits the data into features and labels, which are then\n",
    "    separated into training and testing datasets.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        target (str): The name of the target column in the dataset.\n",
    "        test_size (float): The proportion of the dataset to include in the test split (default is 0.2).\n",
    "        random_state (int or None): The random seed for reproducibility of the split (default is None).\n",
    "        data_train (pd.DataFrame): The feature data for the training set.\n",
    "        labels_train (pd.Series): The labels for the training set.\n",
    "        data_test (pd.DataFrame): The feature data for the testing set.\n",
    "        labels_test (pd.Series): The labels for the testing set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, target: str,  test_size= 0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader instance and splits the dataset into training and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset to be split.\n",
    "            target (str): The name of the target column in the dataset.\n",
    "            test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "            random_state (int or None, optional): The random seed used for shuffling the data. Default is None.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.data_train = None\n",
    "        self.labels_train = None\n",
    "        self.data_test = None\n",
    "        self.labels_test = None\n",
    "\n",
    "        #divide data in train and test sets\n",
    "        self._divide_data()\n",
    "\n",
    "\n",
    "    def _divide_data(self):\n",
    "        \"\"\"\n",
    "        Splits the DataFrame into features (X) and labels (y), and divides them into training and testing sets.\n",
    "\n",
    "        The method uses `train_test_split()` from scikit-learn to randomly split the data into\n",
    "        training and test sets based on the specified `test_size` and `random_state`.\n",
    "\n",
    "        The method sets the following attributes:\n",
    "            - data_train: Feature data for training.\n",
    "            - labels_train: Labels for training.\n",
    "            - data_test: Feature data for testing.\n",
    "            - labels_test: Labels for testing.\n",
    "        \"\"\"\n",
    "        x = self.df.drop(columns=[self.target])\n",
    "        y = self.df[self.target]\n",
    "        x_train, y_train, x_test, y_test = train_test_split(\n",
    "            x, y, test_size=self.test_size,random_state=self.random_state\n",
    "        )\n",
    "        self.data_train  = x_train\n",
    "        self.labels_train = y_train\n",
    "        self.data_test    = x_test\n",
    "        self.labels_test  = y_test\n",
    "        print(\"Data divided successfully.\")\n",
    "\n",
    "\n",
    "    def _drop_columns(self, cols):\n",
    "        \"\"\"\n",
    "        Drops a specified feature columns from both the training and testing datasets.\n",
    "\n",
    "        Args:\n",
    "            cols (str): The list of the columns to be dropped from both the training and testing datasets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_train.drop(cols, axis=1, inplace=True)\n",
    "            self.data_test.drop(cols, axis=1, inplace=True)\n",
    "            print(f\"Feature columns {cols} dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error dropping columns {cols}:\", e)\n",
    "\n",
    "\n",
    "class TripDataAnalizer(DataAnalizer):\n",
    "    def __init__(self, df: pd.DataFrame, target: str, test_size=0.2, random_state=None):\n",
    "        super().__init__(df, target, test_size, random_state)\n",
    "\n",
    "\n",
    "    # Metodo para separar \"tpep_pickup_datetime\" e \"tpep_dropoff_datetime\" em várias features\n",
    "\n",
    "\n",
    "    # Metodo para calcular o tempo de duração da viagem\n",
    "\n",
    "    # Metodo para calcular a velocidade média da viagem"
   ],
   "id": "bd54e66a58708ea7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Data Preprocessing",
   "id": "84b76b142f22b334"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Data Cleaning",
   "id": "52afe9aacf7bd201"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:05:38.444969Z",
     "start_time": "2025-03-08T15:03:50.315025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing implementation\n",
    "\n",
    "# as tabelas vão de \"\\\\2019\\\\2019-01.sqlite\" a \"\\\\2019\\\\2019-12.sqlite\"\n",
    "\n",
    "sql_loader = KagglehubSQLiteLoader(\"dhruvildave/new-york-city-taxi-trips-2019\", \"\\\\2019\\\\2019-01.sqlite\")\n",
    "df = sql_loader.get_table_dataframe_nrows(\"tripdata\", 1000)\n",
    "df\n"
   ],
   "id": "9dd7e1409fe051c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/dhruvildave/new-york-city-taxi-trips-2019?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.38G/2.38G [01:02<00:00, 40.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading dataset: [Errno 28] No space left on device\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Testing implementation\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# as tabelas vão de \"\\\\2019\\\\2019-01.sqlite\" a \"\\\\2019\\\\2019-12.sqlite\"\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m sql_loader \u001B[38;5;241m=\u001B[39m \u001B[43mKagglehubSQLiteLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdhruvildave/new-york-city-taxi-trips-2019\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m2019\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m2019-01.sqlite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m df \u001B[38;5;241m=\u001B[39m sql_loader\u001B[38;5;241m.\u001B[39mget_table_dataframe_nrows(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtripdata\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m      7\u001B[0m df\n",
      "Cell \u001B[1;32mIn[3], line 78\u001B[0m, in \u001B[0;36mKagglehubSQLiteLoader.__init__\u001B[1;34m(self, dataset, file)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03mInitializes the KagglehubSQLLoader class by calling the superclass constructor\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03mto download the dataset and setting up the SQLite connection.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;124;03m    file (str): The name of the SQLite file in the downloaded dataset to be opened.\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(dataset)\n\u001B[1;32m---> 78\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conn \u001B[38;5;241m=\u001B[39m sql\u001B[38;5;241m.\u001B[39mconnect(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file_path)\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conn\u001B[38;5;241m.\u001B[39mcursor()\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EDA (Exploratory Data Analysis)",
   "id": "84b04da689ac609c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a41788ad790c80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
